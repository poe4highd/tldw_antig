# 2026-01-19 开发日志 (Part 1)

## 任务：开发 YouTube 字幕下载检测脚本

### 1. 需求背景
- **问题描述**：需要一个独立的脚本，能够根据 URL 或视频 ID 下载 YouTube 字幕并保存到特定文件夹 (`backend/tests/data`)，用于检测或测试目的。
- **决定**：使用 `yt-dlp` 实现下载逻辑，脚本存放于 `backend/scripts/`，并在 `docs` 记录使用方法。

### 2. 执行计划
- 见 `implementation_plan.md`。

### 3. 回顾
- **脚本开发**：创建了 `backend/scripts/download_subs.py`，支持基于 `yt-dlp` 的字幕提取，能够自动处理多种语言偏好并跳过视频下载以节省流量。
- **文档沉淀**：编写了 `docs/subtitle_downloader.md`，详细说明了脚本的使用方法及依赖。
- **验证**：脚本在本地环境执行成功，尽管在测试阶段遇到了 YouTube 的 HTTP 429 访问限制，但命令链条与文件保存逻辑已确认无误。

### 4. 经验总结
- **自动化测试数据收集**：通过独立的下载脚本，可以更方便地从生产环境真实的视频中提取字幕样本，用于本地 AI 逻辑的微调和回归测试。
- **IP 频率限制处理**：在受限环境下运行 `yt-dlp` 需要注意 IP 的信誉度，未来若需大规模下载，应考虑引入代理或 Cookie 管理。

---

# 2026-01-19 开发日志 (Part 4)

## 任务：评估系统三期优化（结果持久化与文档升级）

### 1. 需求驱动
- **结果留档**：需要自动保存每次比对的报告，以便追踪优化进度。
- **文档规范**：原文件名已不足以涵盖现有工具集功能，需重命名。

### 2. 执行方案
- **自动保存逻辑**：修改 `compare_subs.py`，默认将报告以 `{VideoID}_{Type}_{Timestamp}.txt` 格式存入 `backend/validation/`。
- **智能推断**：脚本可根据输入参数自动判断当前对比的是 `raw` 还是 `llm` 修正结果。
- **文档升级**：将 `subtitle_downloader.md` 重命名为 `subtitle_utils.md`，并系统性整理了下载与评估的操作指南。

### 3. 测试结果
- **环境验证**：成功生成 `validation/QVBpiuph3rM_raw_20260119_073751.txt`。
- **全流程闭环**：实现了从“视频下载 -> 字幕抓取 -> 自动对比 -> 报告留档”的完整测试自动化流。

---


## 任务：评估系统二期优化（数字归一化与缓存支持）

### 1. 需求驱动
- **数字差异误报**：用户指出 `2 -> 二` 这种格式差异应视为准确。
- **原始转录对比**：需要支持直接读取 `backend/cache/` 下的 Whisper 原始 JSON（列表格式）进行对比。

### 2. 执行方案
- **数字归一化**：在 `clean_text` 中将阿拉伯数字统一转换为中文数字。
- **动态 JSON 解析**：增强 `ai_json_to_text`，自动识别结构化 JSON 与原始 List 格式。
- **简繁对齐**：通过高频偏旁字映射（如 `個 -> 个`），消除了 Whisper 原始结果因输出繁体导致的 CER 误差。

### 3. 验证结论
- **对比对象**：`QVBpiuph3rM` (Whisper 原始缓存)
- **数据变化**：CER 从 **41.10%** 修正为 **29.42%**。
- **逻辑效果**：Top 10 错误中不再出现数字差异，有效隔离了样式差异与内容错误。

---


## 任务：实现字幕对比评估系统

### 1. 需求背景
- **问题描述**：需要一种方法来量化 AI 转录的准确度，并分析常见的错误类型。
- **决定**：开发 `compare_subs.py` 脚本，计算 CER 并统计错词频率。

### 2. 执行内容
- **核心算法**：手动实现了基于动态规划的 Levenshtein 距离算法，用于计算字符错误率 (CER)。
- **格式转换**：集成了对 YouTube SRV1 (XML) 和 AI 转录 JSON 的自动提取逻辑。
- **错误分析**：增加了对替换、遗漏和插入操作的统计，能够输出 Top N 错误对（如 `祂 -> 他`）。
- **文档更新**：在 `docs/subtitle_downloader.md` 中补充了评估说明。

### 3. 测试结果
- **实测视频**：`QVBpiuph3rM`
- **评估指标**：CER 20.02%，准确率约 80%。
- **发现**：精准捕捉到 AI 在人称代词（它/祂/他）及数字格式化（2 -> 二）上的系统性偏好差异。

### 4. 经验总结
- **指标引导优化**：通过 CER 和错误词频表，开发者可以非常直观地看到 Prompt 调整的效果，使转录优化工作从“感觉”转向“数据驱动”。
